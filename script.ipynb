{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa480ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db6be0",
   "metadata": {},
   "source": [
    "# General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fd0858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the database: chess_openings\n"
     ]
    }
   ],
   "source": [
    "# Create the connection to PostgreSQL\n",
    "engine = create_engine('postgresql://postgres:123456@localhost:5432/chess_openings')\n",
    "\n",
    "# Simple query to verify the connection\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT current_database();\"))\n",
    "        db_name = result.fetchone()[0]\n",
    "        print(f\"Successfully connected to the database: {db_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89a46f",
   "metadata": {},
   "source": [
    "## Generate fields for the raw_data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd749a78",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('chess_games.csv')\n",
    "    column_names = df.columns.tolist()\n",
    "    print(column_names)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2eeeb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'raw_data' created successfully in database: chess_openings\n"
     ]
    }
   ],
   "source": [
    "# SQL query to create the raw_data table\n",
    "create_raw_data_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS raw_data (\n",
    "    event VARCHAR(50),\n",
    "    white VARCHAR(255),\n",
    "    black VARCHAR(255),\n",
    "    result VARCHAR(10),\n",
    "    utcdate DATE,\n",
    "    utctime TIME,\n",
    "    whiteelo INT,\n",
    "    blackelo INT,\n",
    "    whiteratingdiff INT,\n",
    "    blackratingdiff INT,\n",
    "    eco VARCHAR(10),\n",
    "    opening VARCHAR(255),\n",
    "    timecontrol VARCHAR(50),\n",
    "    termination VARCHAR(50),\n",
    "    an TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_raw_data_sql))\n",
    "        connection.commit()\n",
    "        print(\"Table 'raw_data' created successfully in database: chess_openings\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating the table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab56b88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table 'raw_data' exists in the database.\n"
     ]
    }
   ],
   "source": [
    "# Check if the table exists\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT * FROM information_schema.tables WHERE table_name = 'raw_data';\"))\n",
    "        if result.rowcount > 0:\n",
    "            print(\"The table 'raw_data' exists in the database.\")\n",
    "        else:\n",
    "            print(\"The table 'raw_data' was not created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking the table existence: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f511b",
   "metadata": {},
   "source": [
    "### Populate the table\n",
    "Use of chunks for better insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ffb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! DO NOT EXECUTE ANOTHER TIME -- TAKES TOO LONG\n",
    "\n",
    "# Data insertion to the raw_data table\n",
    "total_rows = sum(1 for line in open('chess_games.csv')) - 1 \n",
    "print(f\"Total rows: {total_rows}\")\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "# Load the CSV file \n",
    "for chunk in pd.read_csv('chess_games.csv', chunksize=chunk_size, low_memory=False):\n",
    "    chunk.columns = [col.lower() for col in chunk.columns]\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            chunk.to_sql('raw_data', con=connection, if_exists='append', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a663ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows inserted into 'raw_data': 6256184\n"
     ]
    }
   ],
   "source": [
    "# Basic query to verify data insertion\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM raw_data;\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"Total rows inserted into 'raw_data': {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying data insertion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12daa692",
   "metadata": {},
   "source": [
    "### Column renaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73581e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns renamed successfully.\n"
     ]
    }
   ],
   "source": [
    "rename_columns_sql = \"\"\"\n",
    "ALTER TABLE raw_data\n",
    "RENAME COLUMN utcdate TO utc_date;\n",
    "\"\"\"\n",
    "\n",
    "rename_columns_sql += \"\"\"\n",
    "ALTER TABLE raw_data\n",
    "RENAME COLUMN utctime TO utc_time;\n",
    "\"\"\"\n",
    "\n",
    "rename_columns_sql += \"\"\"\n",
    "ALTER TABLE raw_data\n",
    "RENAME COLUMN whiteelo TO white_elo;\n",
    "\"\"\"\n",
    "\n",
    "rename_columns_sql += \"\"\"\n",
    "ALTER TABLE raw_data\n",
    "RENAME COLUMN blackelo TO black_elo;\n",
    "\"\"\"\n",
    "\n",
    "rename_columns_sql += \"\"\"\n",
    "ALTER TABLE raw_data\n",
    "RENAME COLUMN whiteratingdiff TO white_rating_diff;\n",
    "\"\"\"\n",
    "\n",
    "rename_columns_sql += \"\"\"\n",
    "ALTER TABLE raw_data\n",
    "RENAME COLUMN blackratingdiff TO black_rating_diff;\n",
    "\"\"\"\n",
    "\n",
    "rename_columns_sql += \"\"\"\n",
    "ALTER TABLE raw_data\n",
    "RENAME COLUMN timecontrol TO time_control;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(rename_columns_sql))\n",
    "        connection.commit()\n",
    "        print(\"Columns renamed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while renaming columns: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da96e1",
   "metadata": {},
   "source": [
    "## Create reconciled dimension tables for the second layer of the DW architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12223edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension tables created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_reconciled_players_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS reconciled_players (\n",
    "    player_id SERIAL PRIMARY KEY,\n",
    "    player_name VARCHAR(255),\n",
    "    player_elo INT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_reconciled_openings_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS reconciled_openings (\n",
    "    opening_id SERIAL PRIMARY KEY,\n",
    "    opening_name VARCHAR(255),\n",
    "    opening_eco VARCHAR(10)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_reconciled_results_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS reconciled_results (\n",
    "    result_id SERIAL PRIMARY KEY,\n",
    "    result_name VARCHAR(10)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_reconciled_players_sql))\n",
    "        connection.execute(text(create_reconciled_openings_sql))\n",
    "        connection.execute(text(create_reconciled_results_sql))\n",
    "        connection.commit()\n",
    "        print(\"Dimension tables created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating dimension tables: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e0150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension tables created: reconciled_players, reconciled_openings, reconciled_results\n"
     ]
    }
   ],
   "source": [
    "# Query to verify the creation of dimension tables\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT table_name FROM information_schema.tables WHERE table_name IN ('reconciled_players', 'reconciled_openings', 'reconciled_results');\"))\n",
    "        tables = [row[0] for row in result.fetchall()]\n",
    "        if tables:\n",
    "            print(f\"Dimension tables created: {', '.join(tables)}\")\n",
    "        else:\n",
    "            print(\"No dimension tables were created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying dimension tables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c8f07f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_data_temp table emptied and ID counter reset to 1\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"TRUNCATE TABLE cleaned_data_temp RESTART IDENTITY CASCADE\"))\n",
    "    connection.commit()\n",
    "    print(\"cleaned_data_temp table emptied and ID counter reset to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdc878",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning process...\n",
      "Processing chunk 1...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 2...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 3...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 4...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 5...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 6...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 7...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 8...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 9...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 10...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 11...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 12...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 13...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 14...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 15...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 16...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 17...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 18...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 19...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 20...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 21...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 22...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 23...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 24...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 25...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 26...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 27...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 28...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 29...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 30...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 31...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 32...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 33...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 34...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 35...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 36...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 37...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 9,999 → After results filter: 9,999\n",
      "Processing chunk 38...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 39...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 40...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 41...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 42...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 43...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 44...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 45...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 46...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 47...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 48...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 49...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 50...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 51...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 52...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 53...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 54...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 55...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 56...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 57...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 58...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 59...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 60...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 61...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 62...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 63...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 64...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 65...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,921\n",
      "Processing chunk 66...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 67...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 68...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 69...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 70...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 71...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 72...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 73...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 74...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 75...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 76...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 77...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 78...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 79...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 80...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 81...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 82...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 83...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 84...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 85...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,931\n",
      "Processing chunk 86...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,834\n",
      "Processing chunk 87...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 88...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 89...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 90...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 91...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 92...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 93...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 94...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 95...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 96...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 97...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 98...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 99...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 100...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 101...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 102...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 103...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 104...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 105...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 106...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 107...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,968\n",
      "Processing chunk 108...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 109...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 110...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 111...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 112...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 113...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 114...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 115...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 116...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 117...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 118...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 119...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 120...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 121...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 122...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 123...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 124...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 125...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 126...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 127...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 128...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 129...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 130...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 131...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 132...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 133...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 134...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 135...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 136...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 137...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 138...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 139...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 140...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 141...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 142...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,745\n",
      "Processing chunk 143...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 144...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 145...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 146...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 147...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 148...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 149...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 150...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 151...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 152...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 153...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 154...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 155...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 156...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 157...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 158...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 159...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 160...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 161...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 162...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 163...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 164...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 165...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,969\n",
      "Processing chunk 166...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 167...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 168...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 169...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 170...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 171...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 172...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 173...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 174...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 175...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 176...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 177...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 178...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 179...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 180...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 181...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 182...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 183...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 184...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 185...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 186...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 187...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 188...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 189...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 190...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 191...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 192...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 193...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 194...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 195...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 196...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 197...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 198...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 199...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 200...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 201...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 202...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 203...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 204...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,970\n",
      "Processing chunk 205...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 206...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 207...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 208...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 209...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 210...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 211...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 212...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 213...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 214...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 215...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 216...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 217...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 218...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 219...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 220...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 221...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 222...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 223...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,969\n",
      "Processing chunk 224...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 225...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,945\n",
      "Processing chunk 226...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 227...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 228...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 229...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 230...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 231...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 232...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 233...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 234...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 235...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 236...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 237...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 238...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 239...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 240...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 241...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 242...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,929\n",
      "Processing chunk 243...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,976\n",
      "Processing chunk 244...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,998\n",
      "Processing chunk 245...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,969\n",
      "Processing chunk 246...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 247...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 248...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 249...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 250...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 251...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 252...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 253...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 254...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 255...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 256...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 257...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 258...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 259...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 260...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 261...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 262...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 263...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 264...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 265...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 266...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 267...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 268...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 269...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 270...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 271...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 272...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 273...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 274...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 275...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 276...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 277...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 278...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 279...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 280...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 281...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 282...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 283...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 284...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 285...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 286...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,981\n",
      "Processing chunk 287...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,975\n",
      "Processing chunk 288...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 289...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 290...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 291...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 292...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 293...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 294...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 295...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 296...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 297...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 298...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 299...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 300...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 301...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 302...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 303...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 304...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,998\n",
      "Processing chunk 305...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,969\n",
      "Processing chunk 306...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 307...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 308...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 309...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 310...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 311...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 312...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 313...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 314...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 315...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 316...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 317...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 318...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 319...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 320...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 321...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,974\n",
      "Processing chunk 322...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 323...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 324...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 325...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 326...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 327...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 328...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 329...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 330...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 331...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 332...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 333...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 334...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 335...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 336...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 337...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 338...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 339...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 340...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 341...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 342...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 343...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 344...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 345...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 346...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 347...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 348...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 349...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 350...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 351...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 352...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 353...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 354...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 355...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 356...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 357...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 358...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 359...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 360...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 361...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 362...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 363...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 364...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,987\n",
      "Processing chunk 365...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,984\n",
      "Processing chunk 366...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 367...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 368...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 369...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 370...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 371...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 372...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 373...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 374...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 375...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 376...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 377...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 378...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 379...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 380...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 381...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 382...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 383...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 384...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 385...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,997\n",
      "Processing chunk 386...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,985\n",
      "Processing chunk 387...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 388...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 389...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 390...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 391...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 392...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 393...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 394...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 395...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 396...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 397...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 398...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 399...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 400...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 401...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 402...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 403...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 404...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 405...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,983\n",
      "Processing chunk 406...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 407...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 408...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 409...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 410...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 411...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 412...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 413...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 414...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 415...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 416...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 417...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 418...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 419...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 420...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 421...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 422...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 423...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 424...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 425...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 426...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 427...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,983\n",
      "Processing chunk 428...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 429...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 430...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 431...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 432...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 433...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 434...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 435...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 436...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 437...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 438...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 439...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 440...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 441...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 442...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 443...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 444...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 445...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 446...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 447...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 448...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 449...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 450...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 451...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 452...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 453...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 454...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 455...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 456...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 457...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 458...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 459...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 460...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 461...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 462...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 463...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 464...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 465...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 466...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 467...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 468...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 469...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 470...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 471...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 472...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 473...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 474...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 475...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 476...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 477...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 478...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 479...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 480...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 481...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 482...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 483...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 484...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 485...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 486...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,984\n",
      "Processing chunk 487...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,998\n",
      "Processing chunk 488...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,889\n",
      "Processing chunk 489...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 490...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 491...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,972\n",
      "Processing chunk 492...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 493...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 494...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 495...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 496...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 497...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 498...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 499...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 500...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 501...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 502...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 503...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 504...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 505...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 506...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 507...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 508...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,988\n",
      "Processing chunk 509...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,989\n",
      "Processing chunk 510...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 511...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 512...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 513...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 514...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 515...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 516...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 517...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 518...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 519...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 520...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 521...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 522...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 523...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 524...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 525...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 526...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 527...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 528...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,995\n",
      "Processing chunk 529...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,987\n",
      "Processing chunk 530...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 531...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 532...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 533...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 534...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 535...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 536...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 537...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 538...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 539...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 540...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 541...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 542...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 543...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 544...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 545...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 546...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 547...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 548...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 549...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,998\n",
      "Processing chunk 550...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,988\n",
      "Processing chunk 551...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 552...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 553...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 554...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 555...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 556...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 557...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 558...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 559...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 560...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 561...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 562...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 563...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 564...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 565...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 566...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 567...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 568...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 569...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 570...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 571...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,989\n",
      "Processing chunk 572...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,985\n",
      "Processing chunk 573...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 574...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 575...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 576...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 577...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 578...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 579...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 580...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 581...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 582...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 583...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 584...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 585...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 586...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 587...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 588...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 589...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 590...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 591...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,988\n",
      "Processing chunk 592...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 593...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 594...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 595...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 596...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 597...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 598...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 599...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 600...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 601...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 602...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 603...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 604...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 605...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 606...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 607...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 608...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 609...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 610...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 611...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,986\n",
      "Processing chunk 612...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 613...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 614...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 615...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 616...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 617...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 618...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 9,999\n",
      "Processing chunk 619...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 620...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 621...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 622...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 623...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 624...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 625...\n",
      "  Initial: 10,000 → After nulls: 10,000 → After duplicates: 10,000 → After results filter: 10,000\n",
      "Processing chunk 626...\n",
      "  Initial: 6,184 → After nulls: 6,184 → After duplicates: 6,184 → After results filter: 6,184\n",
      "Data cleaning completed!\n"
     ]
    }
   ],
   "source": [
    "#! DO NOT EXECUTE ANOTHER TIME -- TAKES TOO LONG\n",
    "\n",
    "# Data cleaning process before inserting them into the reconciled tables\n",
    "chunk_size = 10000\n",
    "cleaned_chunks = []\n",
    "\n",
    "print(\"Starting data cleaning process...\")\n",
    "\n",
    "for i, chunk in enumerate(pd.read_sql('SELECT * FROM raw_data', engine, chunksize=chunk_size)):\n",
    "    print(f\"Processing chunk {i+1}...\")\n",
    "    \n",
    "    # 1. Drop rows with null values on key columns\n",
    "    initial_count = len(chunk)\n",
    "    chunk = chunk.dropna(subset=['white', 'black', 'white_elo', 'black_elo', 'eco', 'opening', 'result'])\n",
    "    after_nulls = len(chunk)\n",
    "    \n",
    "    # 2. Drop duplicate rows\n",
    "    chunk = chunk.drop_duplicates()\n",
    "    after_duplicates = len(chunk)\n",
    "    \n",
    "    # 3. Normalize player names\n",
    "    chunk['white'] = chunk['white'].str.strip().str.lower()\n",
    "    chunk['black'] = chunk['black'].str.strip().str.lower()\n",
    "    \n",
    "    # 4. Filter only standardized results\n",
    "    valid_results = ['1-0', '0-1', '1/2-1/2']\n",
    "    chunk = chunk[chunk['result'].isin(valid_results)]\n",
    "    after_results = len(chunk)\n",
    "    \n",
    "    # 5. Normalize ECO code\n",
    "    chunk['eco'] = chunk['eco'].str.strip().str.upper()\n",
    "    \n",
    "    # 6. Normalize openings\n",
    "    chunk['opening'] = chunk['opening'].str.strip().str.title()\n",
    "    \n",
    "    # 7. Clean and normalize event field\n",
    "    chunk['event'] = chunk['event'].str.strip().str.lower()\n",
    "    chunk.loc[chunk['event'].str.contains('blitz', na=False), 'event'] = 'Blitz'\n",
    "    chunk.loc[chunk['event'].str.contains('bullet', na=False), 'event'] = 'Bullet'\n",
    "    chunk.loc[chunk['event'].str.contains('classical', na=False), 'event'] = 'Classical'\n",
    "    chunk.loc[chunk['event'].str.contains('correspondence', na=False), 'event'] = 'Correspondence'\n",
    "    \n",
    "    # 8. Consider only normal termination\n",
    "    chunk['termination'] = chunk['termination'].astype(str).str.strip()\n",
    "    chunk = chunk[chunk['termination'] == 'Normal']\n",
    "\n",
    "    print(f\"  Initial: {initial_count:,} → After nulls: {after_nulls:,} → After duplicates: {after_duplicates:,} → After results filter: {after_results:,}\")\n",
    "    \n",
    "    # Store cleaned chunk temporarily in database\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            chunk.to_sql('cleaned_data_temp', con=connection, if_exists='append', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing cleaned chunk {i+1}: {e}\")\n",
    "\n",
    "print(\"Data cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5b3baf7",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows inserted into 'cleaned_data_temp': 6254840\n",
      "Total rows on 'raw_data': 6256184\n"
     ]
    }
   ],
   "source": [
    "# Basic query to verify data insertion into cleaned_data_temp\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM cleaned_data_temp;\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"Total rows inserted into 'cleaned_data_temp': {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying data insertion: {e}\")\n",
    "\n",
    "# Compare it to raw_data\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM raw_data;\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"Total rows on 'raw_data': {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying data insertion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "960108c6",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing dimension tables...\n",
      "Dimension tables cleared successfully!\n"
     ]
    }
   ],
   "source": [
    "# Clear dimension tables before populating\n",
    "print(\"Clearing dimension tables...\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"TRUNCATE TABLE reconciled_players, reconciled_openings, reconciled_results RESTART IDENTITY CASCADE;\"))\n",
    "        connection.commit()\n",
    "    print(\"Dimension tables cleared successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error clearing tables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e4757",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Populate reconciled_players\n",
    "print(\"Populating players dimension...\")\n",
    "\n",
    "chunk_size = 10000\n",
    "total_players_inserted = 0\n",
    "unique_players_set = set()\n",
    "\n",
    "try:\n",
    "    # Process cleaned_data_temp in chunks\n",
    "    for i, chunk in enumerate(pd.read_sql('SELECT white, white_elo, black, black_elo FROM cleaned_data_temp', engine, chunksize=chunk_size)):\n",
    "        print(f\"Processing chunk {i+1} for players dimension...\")\n",
    "        \n",
    "        # Extract white players from current chunk\n",
    "        players_white = chunk[['white', 'white_elo']].copy()\n",
    "        players_white.columns = ['player_name', 'player_elo']\n",
    "        \n",
    "        # Extract black players from current chunk\n",
    "        players_black = chunk[['black', 'black_elo']].copy()\n",
    "        players_black.columns = ['player_name', 'player_elo']\n",
    "        \n",
    "        # Combine white and black players from this chunk\n",
    "        chunk_players = pd.concat([players_white, players_black]).drop_duplicates()\n",
    "        \n",
    "        # Filter out players we've already seen (to avoid duplicates across chunks)\n",
    "        new_players = []\n",
    "        for _, player in chunk_players.iterrows():\n",
    "            player_key = (player['player_name'], player['player_elo'])\n",
    "            if player_key not in unique_players_set:\n",
    "                unique_players_set.add(player_key)\n",
    "                new_players.append(player)\n",
    "        \n",
    "        if new_players:\n",
    "            new_players_df = pd.DataFrame(new_players)\n",
    "            \n",
    "            # Insert new players into reconciled_players\n",
    "            with engine.connect() as connection:\n",
    "                new_players_df.to_sql('reconciled_players', con=connection, if_exists='append', index=False, method='multi')\n",
    "            \n",
    "            total_players_inserted += len(new_players_df)\n",
    "            print(f\"  Inserted {len(new_players_df):,} new players from chunk {i+1}\")\n",
    "        else:\n",
    "            print(f\"  No new players found in chunk {i+1}\")\n",
    "\n",
    "    print(f\"\\nPlayers dimension populated! Total unique players inserted: {total_players_inserted:,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error populating players dimension: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c24014a",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconciled_players: 6276377 records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Simple verification for reconciled_players\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM reconciled_players\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"reconciled_players: {count} records inserted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84c087",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table columns:\n",
      "('opening_id', 'integer', 'NO', \"nextval('reconciled_openings_opening_id_seq'::regclass)\")\n",
      "('opening_name', 'character varying', 'YES', None)\n",
      "('opening_eco', 'character varying', 'YES', None)\n",
      "\n",
      "Constraints:\n",
      "('PRIMARY KEY', 'opening_id')\n"
     ]
    }
   ],
   "source": [
    "# POpulate reconciled_openings\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"\"\"\n",
    "            INSERT INTO reconciled_openings (opening_eco, opening_name)\n",
    "            SELECT DISTINCT eco, opening\n",
    "            FROM cleaned_data_temp\n",
    "            WHERE eco IS NOT NULL AND opening IS NOT NULL\n",
    "            ORDER BY eco, opening\n",
    "        \"\"\"))\n",
    "        connection.commit()\n",
    "        count = connection.execute(text(\"SELECT COUNT(*) FROM reconciled_openings\")).fetchone()[0]\n",
    "        print(f\"reconciled_openings: {count} records inserted starting from ID 1\")\n",
    "except Exception as e:\n",
    "    print(f\"Error populating openings dimension: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07aa417b",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   opening_id                                       opening_name opening_eco\n",
      "0        1421  French Defense: Tarrasch Variation, Chistyakov...         C07\n",
      "1        1365                  French Defense: Pelikan Variation         C00\n",
      "2        2679            Semi-Slav Defense: Semi-Meran Variation         D47\n",
      "3        1572                  Bishop'S Opening: Lopez Variation         C23\n",
      "4        1082    Sicilian Defense: Closed Variation, Traditional         B25\n",
      "5        2836            Bogo-Indian Defense: Vitolins Variation         E11\n",
      "6         687  Nimzowitsch Defense: Kennedy Variation, Rieman...         B00\n",
      "7         209              King'S Indian Attack: Keres Variation         A07\n",
      "8        2432                    Queen'S Pawn Game: Torre Attack         D03\n",
      "9        2335  Ruy Lopez: Morphy Defense, Breyer Defense, Zai...         C95\n"
     ]
    }
   ],
   "source": [
    "# Quick verification for reconciled_openings\n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql(\"SELECT DISTINCT * FROM reconciled_openings LIMIT 10\", connection)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25f670be",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating results dimension...\n"
     ]
    }
   ],
   "source": [
    "print(\"Populating results dimension...\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"\"\"\n",
    "            INSERT INTO reconciled_results (result)\n",
    "            SELECT DISTINCT result\n",
    "            FROM cleaned_data_temp\n",
    "            WHERE result IS NOT NULL\n",
    "            ORDER BY result\n",
    "        \"\"\"))\n",
    "        connection.commit()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error populating results dimension: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd1b47c8",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconciled_results: 3 records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Simple verification for reconciled_results\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM reconciled_results\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"reconciled_results: {count} records inserted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab44dd41",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_games table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Fact table creation\n",
    "create_fact_games_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_games (\n",
    "    game_id SERIAL PRIMARY KEY,\n",
    "    white_player_id INT NOT NULL REFERENCES reconciled_players(player_id),\n",
    "    black_player_id INT NOT NULL REFERENCES reconciled_players(player_id),\n",
    "    opening_id INT NOT NULL REFERENCES reconciled_openings(opening_id),\n",
    "    result_id INT NOT NULL REFERENCES reconciled_results(result_id),\n",
    "    event TEXT,\n",
    "    utc_date DATE,\n",
    "    utc_time TIME,\n",
    "    white_elo INT,\n",
    "    black_elo INT,\n",
    "    white_rating_diff INT,\n",
    "    black_rating_diff INT,\n",
    "    time_control TEXT,\n",
    "    termination TEXT,\n",
    "    an TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_fact_games_sql))\n",
    "        connection.commit()\n",
    "    print(\"fact_games table created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating fact_games table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fff1a",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating fact_games table...\n",
      "fact_games: 6254840 records inserted starting from ID 1\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(text(\"TRUNCATE TABLE fact_games RESTART IDENTITY CASCADE\"))\n",
    "    connection.commit()\n",
    "\n",
    "print(\"Populating fact_games table...\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"\"\"\n",
    "            INSERT INTO fact_games (\n",
    "                white_player_id,\n",
    "                black_player_id,\n",
    "                opening_id,\n",
    "                result_id,\n",
    "                event,\n",
    "                utc_date,\n",
    "                utc_time,\n",
    "                white_elo,\n",
    "                black_elo,\n",
    "                white_rating_diff,\n",
    "                black_rating_diff,\n",
    "                time_control,\n",
    "                termination,\n",
    "                an\n",
    "            )\n",
    "            SELECT\n",
    "                wp.player_id AS white_player_id,\n",
    "                bp.player_id AS black_player_id,\n",
    "                o.opening_id,\n",
    "                r.result_id,\n",
    "                c.event,\n",
    "                c.utc_date,\n",
    "                c.utc_time,\n",
    "                c.white_elo,\n",
    "                c.black_elo,\n",
    "                c.white_rating_diff,\n",
    "                c.black_rating_diff,\n",
    "                c.time_control,\n",
    "                c.termination,\n",
    "                c.an\n",
    "            FROM cleaned_data_temp c\n",
    "            JOIN reconciled_players wp\n",
    "                ON c.white = wp.player_name AND c.white_elo = wp.player_elo\n",
    "            JOIN reconciled_players bp\n",
    "                ON c.black = bp.player_name AND c.black_elo = bp.player_elo\n",
    "            JOIN reconciled_openings o\n",
    "                ON c.eco = o.opening_eco AND c.opening = o.opening_name\n",
    "            JOIN reconciled_results r\n",
    "                ON c.result = r.result\n",
    "        \"\"\"))\n",
    "        connection.commit()\n",
    "        count = connection.execute(text(\"SELECT COUNT(*) FROM fact_games\")).fetchone()[0]\n",
    "        print(f\"fact_games: {count} records inserted starting from ID 1\")\n",
    "except Exception as e:\n",
    "    print(f\"Error populating fact_games table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e0859c7",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   game_id  white_player_id  black_player_id  opening_id  result_id  \\\n",
      "0        1              569            10319         940          1   \n",
      "1        2              569           560820        1967          1   \n",
      "2        3              970            10711         394          2   \n",
      "3        4             1225            10956        1141          2   \n",
      "4        5             1225          4289990        1225          2   \n",
      "5        6             2548            12222        2987          1   \n",
      "6        7             3108            12755        1531          1   \n",
      "7        8             3126          2616133        1360          1   \n",
      "8        9             3126           301773         912          2   \n",
      "9       10             3126          1398318        1921          1   \n",
      "\n",
      "                event    utc_date  utc_time  white_elo  black_elo  \\\n",
      "0              Blitz   2016-06-30  22:02:48       1485       1902   \n",
      "1              Blitz   2016-07-04  22:02:43       1485       1624   \n",
      "2              Blitz   2016-06-30  22:05:17       2093       2121   \n",
      "3              Blitz   2016-06-30  22:06:46       1825       1704   \n",
      "4              Blitz   2016-07-25  20:58:33       1825       1638   \n",
      "5              Blitz   2016-06-30  22:14:36       2333       2333   \n",
      "6   Blitz tournament   2016-06-30  22:18:05       1188       1641   \n",
      "7             Bullet   2016-07-20  22:31:45       1700       1758   \n",
      "8             Bullet   2016-07-25  15:19:24       1700       1658   \n",
      "9             Bullet   2016-07-05  16:46:04       1700       2037   \n",
      "\n",
      "   white_rating_diff  black_rating_diff time_control   termination  \\\n",
      "0                 -2                  2        300+3        Normal   \n",
      "1                 -8                  7        300+3  Time forfeit   \n",
      "2                 12                -11        180+0  Time forfeit   \n",
      "3                  7                 -8        180+0        Normal   \n",
      "4                  6                 -7        180+0        Normal   \n",
      "5                -10                 10        180+2        Normal   \n",
      "6                 -3                  1        300+0        Normal   \n",
      "7                 -9                  9         60+0  Time forfeit   \n",
      "8                 10                 -8         60+0  Time forfeit   \n",
      "9                 -3                  3         60+0        Normal   \n",
      "\n",
      "                                                  an  \n",
      "0  1. e4 c6 2. d4 d5 3. e5 Bf5 4. Bd3 Bxd3 5. Qxd...  \n",
      "1                 1. e4 e5 2. Nf3 Nc6 3. Nc3 Nf6 0-1  \n",
      "2  1. d4 g6 2. Nf3 Bg7 3. Bf4 d6 4. e3 Nf6 5. h3 ...  \n",
      "3  1. e4 c5 2. Nf3 Nc6 3. d4 cxd4 4. Nxd4 Nxd4 5....  \n",
      "4  1. e4 { [%eval 0.31] } 1... c5 { [%eval 0.26] ...  \n",
      "5  1. d4 Nf6 2. Nf3 g6 3. g3 Bg7 4. Bg2 O-O 5. O-...  \n",
      "6  1. e4 e5 2. d4 Qf6 3. dxe5 Qxe5 4. Qe2 Bc5 5. ...  \n",
      "7  1. e4 e6 2. Nf3 d5 3. exd5 exd5 4. d4 Nf6 5. N...  \n",
      "8  1. e4 c6 2. Nf3 d5 3. exd5 cxd5 4. d4 Nf6 5. B...  \n",
      "9  1. e4 e5 2. Nf3 Nc6 3. d4 f5 4. dxe5 fxe4 5. N...  \n"
     ]
    }
   ],
   "source": [
    "# Quick verification for fact_games\n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql(\"SELECT * FROM fact_games ORDER BY game_id LIMIT 10\", connection)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5ec3b91",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA INTEGRITY CHECK ===\n",
      "Missing white players: 0\n",
      "Missing black players: 0\n",
      "Missing openings: 0\n",
      "Missing results: 0\n",
      "All integrity checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Check data integrity in fact_games\n",
    "print(\"=== DATA INTEGRITY CHECK ===\")\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    # Check for missing white_player_id\n",
    "    missing_white = connection.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) AS missing_white_players\n",
    "        FROM fact_games f\n",
    "        LEFT JOIN reconciled_players p ON f.white_player_id = p.player_id\n",
    "        WHERE p.player_id IS NULL\n",
    "    \"\"\")).fetchone()[0]\n",
    "    \n",
    "    # Check for missing black_player_id\n",
    "    missing_black = connection.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) AS missing_black_players\n",
    "        FROM fact_games f\n",
    "        LEFT JOIN reconciled_players p ON f.black_player_id = p.player_id\n",
    "        WHERE p.player_id IS NULL\n",
    "    \"\"\")).fetchone()[0]\n",
    "    \n",
    "    # Check for missing opening_id\n",
    "    missing_openings = connection.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) AS missing_openings\n",
    "        FROM fact_games f\n",
    "        LEFT JOIN reconciled_openings o ON f.opening_id = o.opening_id\n",
    "        WHERE o.opening_id IS NULL\n",
    "    \"\"\")).fetchone()[0]\n",
    "    \n",
    "    # Check for missing result_id\n",
    "    missing_results = connection.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) AS missing_results\n",
    "        FROM fact_games f\n",
    "        LEFT JOIN reconciled_results r ON f.result_id = r.result_id\n",
    "        WHERE r.result_id IS NULL\n",
    "    \"\"\")).fetchone()[0]\n",
    "    \n",
    "    print(f\"Missing white players: {missing_white}\")\n",
    "    print(f\"Missing black players: {missing_black}\")\n",
    "    print(f\"Missing openings: {missing_openings}\")\n",
    "    print(f\"Missing results: {missing_results}\")\n",
    "    \n",
    "    if missing_white + missing_black + missing_openings + missing_results == 0:\n",
    "        print(\"All integrity checks passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
